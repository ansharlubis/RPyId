---
title: "Data is beautiful part 1 - Hello Tidyverse! : R Series"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**3rdMeeting - Sunday, December 15, 2019**

**Tutors and Organizers:** 

- Pande Putu Erawijantari
- Felix Salim
- Mia Fitria Utami

## Content:

- Introductions about R packages and libraries
- Data Wrangling session

Sources: 

- Introduction of R packages and libraries
  - <https://r4ds.had.co.nz/explore-intro.html>
  - <https://support.rstudio.com/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages>

- Data Wrangling session:
  - <https://datacarpentry.org/R-ecology-lesson/03-dplyr.html>

- Data:

source: <https://ndownloader.figshare.com/files/2292169>

Execute the command below if you miss the previous meeting.

```
dir.create("data_raw")

download.file(url="https://ndownloader.figshare.com/files/2292169",
              destfile = "data_raw/portal_data_joined.csv")
```

### Introductions about R packages and libraries

In the previous meeting we have learned how to analyse multiple datasets using a functions. However, sometimes we need to analyse complex data which may require complex functions. Don't worry, many useful R function come in packages or libraries. In R **packages** are collection of R functions, data, and compiled code written by R's active user community in a well-defined format. The directory where packagase are stored is call **library**.

To install an R package, you can open an R session in your R studio and type at the command line

```install.packages("<the package's name>")```

R will download the package from [CRAN](https://cran.r-project.org/), so you will need to be connected to the internet. It should be noted that the package instalation only needed if you haven't done it yet, usually when you use a package for the first time. To use the installed-package, you have to call it beforehand. You can make its contents available to use in your current R session by running the command below (You may found it is litle bit counter intuitive to call the packages by `library()` command).

```library("<the package's name>")```

You can see the quick list of useful R packages [here](https://support.rstudio.com/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages).
For our **Data is beautiful: R series**, we will several packages named [dplyr](https://blog.rstudio.com/2014/01/17/introducing-dplyr/), [tidyr](http://blog.rstudio.org/2014/07/22/introducing-tidyr/), and [ggplot2](https://ggplot2.tidyverse.org/reference/) to name of few. There are also package that contains collections of several packages, so you don't need to install it one by one. 

[**Tidyvere**](https://www.tidyverse.org/) is one of the example, which contains a collection of R packages designed for data science. All package share similar design, grammar, and data structure so that you can use it all together without worying about the data structure. All of the package we need for our next session is available in **Tidyverse**, so let's install it!

```install.packages("tidyverse")```

### Data Wrangling Session

**TIPS**: If you want to learn how to type the code properly, remember the structure for long run, and become sensitive of the typo error or understanding error message, **DO NOT copy paste the example**. But if you already know how it works, than it's ok to copy paste, as you are not supposed to remember every exact syntax. Copy paste at your own risk! 

ref: <https://datacarpentry.org/R-ecology-lesson/03-dplyr.html>
We have start looking at the data in the previous meeting but we haven't analyze it yet.
Let's get your hand ready!

#### Data manipulation using **dyplyr** and **tidyr**

Previously, we learn to parse our data using bracket, but it can be difficult to read especially if you start using complicated operations. We will use **dyplyr** and **tidyr** to help us through. **dplyr** is a package for making tabular data manipulation easier. It pairs nicely with **tidyr** which enables us to swiftly convert between different data formats for plotting and analysis.  To learn more about **dyplyr** and **tidyr**, you may want to check out [dyplyr cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf) and [tidyr cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf). 

As mentioned before, it all installed together during the **tidyverse** installation. Let's load the package

```{r}
## load the tidyverse packages, incl. dplyr
library("tidyverse")
```

We’ll read in our data using the `read_csv()` function, from the tidyverse package readr, instead of `read.csv()` (notice the underscore(_) vs point(.)?).

```{r}
surveys <- read_csv("data_raw/portal_data_joined.csv")
```

```{r}
## inspect the data
str(surveys)
```

```
## preview the data
View(surveys)
```

Notice that the class of the data is now `tbl_df`

This is referred to as a “tibble”. Tibbles tweak some of the behaviors of the data frame objects we introduced in the previous episode. The data structure is very similar to a data frame. For our purposes the only differences are that:

1. In addition to displaying the data type of each column under its name, it only prints the first few rows of data and only as many columns as fit on one screen.
2. Columns of class character are never converted into factors.

We’re going to learn some of the most common dplyr functions:

  * `select()`: subset columns
  * `filter()`: subset rows on conditions
  * `mutate()`: create new columns by using information from other columns
  * `group_by()`: and summarize(): create summary statisitcs on grouped data
  * `arrange()` : sort results
  * `count()`: count discrete values

#### Selecting column and filtering rows

To select columns of a data frame, use select(). The first argument to this function is the data frame (surveys), and the subsequent arguments are the columns to keep.

```{r,colapse=TRUE}
select(surveys, plot_id, species_id, weight)
```

To select all columns except certain ones, put a “-” in front of the variable to exclude it.
```{r,colapse=TRUE}
select(surveys, -record_id, -species_id)
```

This will select all the variables in surveys except record_id and species_id.

To choose rows based on a specific criteria, use filter():
```{r,collapse=TRUE}
filter(surveys, year == 1995)
```


#### Pipes

What if you want to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, or pipes.

With intermediate steps, you create a temporary data frame and use that as input to the next function, like this:

```{r,collapse=TRUE}
surveys2 <- filter(surveys, weight < 5)
surveys_sml <- select(surveys2, species_id, sex, weight)
```

This is readable, but can clutter up your workspace with lots of objects that you have to name individually. With multiple steps, that can be hard to keep track of.

You can also nest functions (i.e. one function inside of another), like this:

```{r,collapse=TRUE}
surveys_sml <- select(filter(surveys, weight < 5), species_id, sex, weight)
```

This is handy, but can be difficult to read if too many functions are nested, as R evaluates the expression from the inside out (in this case, filtering, then selecting).

The last option, pipes, are a recent addition to R, for those who familiar with bash scripting probably has experience how to use the pipe.

Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like `%>%` (in bash is `|`) and are made available via the `magrittr` package, installed automatically with `dplyr`. If you use RStudio, you can type the pipe with **Ctrl** + **Shift** + **M** in Windows or **Cmd** + **Shift** + **M** if you have a Mac.

```{r,collapse=TRUE}
surveys %>% 
  filter(weight < 5) %>% 
  select(species_id, sex, weight)
```

In the above code, we use the pipe to send the surveys dataset first through `filter()` to keep rows where weight is less than 5, then through `select()` to keep only the species_id, sex, and weight columns. Since `%>%` takes the object on its left and passes it as the first argument to the function on its right, we don’t need to explicitly include the data frame as an argument to the `filter()` and `select()` functions any more.

If we want to create a new object with this smaller version of the data, we can assign it a new name:

```{r,collapse=TRUE}
surveys_sml <- surveys %>% 
  filter(weight < 5) %>% 
  select(species_id, sex, weight)
surveys_sml
```

**Challenge 1 (10 min) **

>
Using pipes, subset the surveys data to include animals collected before 1995 and retain only the columns year, sex, and weight.

#### Mutate

Frequently you’ll want to create new columns based on the values in existing columns, for example to do unit conversions, value normalization or to find the ratio of values in two columns. For this we’ll use mutate().

To create a new column of weight in kg:

```{r,collapse=TRUE}
surveys %>% 
  mutate(weight_kg = weight / 1000)
```

You can also create a second new column based on the first new column within the same call of `mutate()`. If this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. 

```{r}
surveys %>% 
  mutate(weight_kg = weight / 1000,
         weight_lb = weight_kg * 2.2) %>% 
  head()
```

The first few rows of the output are full of NAs, so if we wanted to remove those we could insert a filter() in the chain:

```{r}
surveys %>% 
  filter(!is.na(weight)) %>% 
  mutate(weight_kg = weight / 1000) %>% 
  head()
```

`is.na()` is a function that determines whether something is an **NA**. The `!` symbol negates the result, so we’re asking for every row where weight is not an NA.

**Challenge 2 (15 min)**

>
Create a new data frame from the surveys data that meets the following criteria: contains only the species_id column and a new column called hindfoot_half containing values that are half the hindfoot_length values. In this hindfoot_half column, there are no NAs and all values are less than 30.
Hint: think about how the commands should be ordered to produce this data frame!


#### Split-apply-combine data analysis and the summarize() function

Many data analysis tasks can be approached using the *split-apply-combine* paradigm: split the data into groups, apply some analysis to each group, and then combine the results. **dplyr** makes this very easy through the use of the `group_by()` function.

`group_by()` is often used together with summarize(), which collapses each group into a single-row summary of that group. `group_by()` takes as arguments the column names that contain the categorical variables for which you want to calculate the summary statistics. So to compute the mean weight by sex:

```{r,collapse=TRUE}
surveys %>% 
  group_by(sex) %>% 
  summarise(mean_weight = mean(weight,na.rm=TRUE))
```

You may also have noticed that the output from these calls doesn’t run off the screen anymore. It’s one of the advantages of `tbl_df` over data frame.

You can also group by multiple columns:

```{r,collapse=TRUE}
surveys %>% 
  group_by(sex, species_id) %>% 
  summarise(mean_weight = mean(weight,na.rm=TRUE))
```

When grouping both by sex and species_id, the last few rows are for animals that escaped before their sex and body weights could be determined. You may notice that the last column does not contain NA but NaN (which refers to “Not a Number”). To avoid this, we can remove the missing values for weight before we attempt to calculate the summary statistics on weight. Because the missing values are removed first, we can omit na.rm = TRUE when computing the mean:

```{r,collapse=TRUE}
surveys %>%
  filter(!is.na(weight)) %>% 
  group_by(sex, species_id) %>% 
  summarise(mean_weight = mean(weight))
```


Once the data are grouped, you can also summarize multiple variables at the same time (and not necessarily on the same variable). For instance, we could add a column indicating the minimum weight for each species for each sex. We also can sort the value for example based on min_weight using `arrange()`

```{r,collapse=TRUE}
surveys %>%
  filter(!is.na(weight)) %>% 
  group_by(sex, species_id) %>% 
  summarise(mean_weight = mean(weight),
            min_weight = min(weight)) %>% 
  arrange(min_weight)
```

To sort in descending order, we need to add the desc() function. If we want to sort the results by decreasing order of mean weight.

When working with data, we often want to know the number of observations found for each factor or combination of factors. For this task, dplyr provides `count()`. For example, if we wanted to count the number of rows of data for each sex and sort it, we would do:

```{r,collapse=TRUE}
surveys %>% 
  count(sex,sort=TRUE)
```

Previous example shows the use of `count()` to count the number of rows/observations for one factor (i.e., sex). If we wanted to count combination of factors, such as sex and species, we would specify the first and the second factor as the arguments of `count()`:

```{r,collapse=TRUE}
surveys %>% 
  count(sex, species_id)
```

With the above code, we can proceed with `arrange()` to sort the table according to a number of criteria so that we have a better comparison. For instance, we might want to arrange the table above in (i) an alphabetical order of the levels of the species and (ii) in descending order of the count:

```{r,collapse=TRUE}
surveys %>% 
  count(sex, species_id) %>% 
  arrange(species_id, desc(n))
```


**Challenge 3 (20 min)**

>
1. How many animals were caught in plot_type surveyed?
2. Use group_by() and summarize() to find the mean, min, and max hindfoot length for each species (using species_id). Also add the number of observations (hint: see ?n).
3. What was the heaviest animal measured in each year? Return the columns year, genus, species_id, and weight.

#### Reshaping the data - Tidying up the data

We have to learn the concept of **Data Tidying** in R to better handle our data before processing to analysis such as visualizations, statistical test, aplying functions, and others. We will use **tidyr** for tidying up our data. 

**Key to remember: Tidy data format**

>> 
1. Every column is variable.
2. Every row is an observation.
3. Every cell is a single value.


Sometimes, we just want to analyze particular element of the data. To extract the informations and still make the data tidy, we can reshape our data according to the observations of interest which could be in **long** format or **wide** format. You may want to read more [here](https://tidyr.tidyverse.org/articles/tidy-data.html).

In the previous versions, there are two `tidyr` functions named `spread()` and `gather()` to help us reshaping our data. In the [data carperntry example](https://datacarpentry.org/R-ecology-lesson/03-dplyr.html), you can see how we can use that functions. In newer version, there are replacement for `spread()` and `gather()`. `spread()` and `gather()` will stay there but not in active development. Today, we will use the replacement named `pivot_wider` and `pivot_longer`.

**Convert long format to wide format**

*Case*: compare the different mean weight of each genus between plots? (Ignoring plot_type for simplicity).

*Solution*: extract the table so it will contains the name of `genus` in the columns, rows is the onservations, and cells contains the mean weight for each genus.

Let’s use `pivot_wider` to transform surveys to find the mean weight of each genus in each plot over the entire survey period. We use `filter()`, `group_by()` and `summarise()` to filter our observations and variables of interest, and create a new variable for the mean_weight. We use the pipe as before too.

```{r,collapse=TRUE}
surveys_gw <- surveys %>% 
  filter(!is.na(weight)) %>% 
  group_by(genus, plot_id) %>% 
  summarize(mean_weight = mean(weight))

str(surveys_gw)
```

This yields `surveys_gw` where the observations for each plot are spread across multiple rows, 196 observations of 3 variables. `pivot_wider` will "widens" data, increasing the number of columns and decreasing the number of rows. We will get 24 observations of 11 variables, one row for each plot. We again use pipes.  Let's also fill the mising value with 0


```{r,collapse=TRUE}
surveys_wider <- surveys_gw %>% 
  pivot_wider(names_from= genus, values_from= mean_weight, 
              values_fill = list(mean_weight=0))

str(surveys_wider)
```


**Convert wide format to long format**

The opposing situation could occur if we had been provided with data in the form of `surveys_wide`, where the genus names are column names, but we wish to treat them as values of a genus variable instead.

In this situation we can reshape it by Keying the column names and turning them into a pair of new variables. One variable represents the column names as values, and the other variable contains the values previously associated with the column names.


To recreate `surveys_gw` from `surveys_wide` we would create a key called "genus" and value called "mean_weight" and use all columns except "plot_id" for the key variable. Here we drop "plot_id" column with a minus sign. We will name our new table as surveys_longer.

```{r,collapse=TRUE}
surveys_longer <- surveys_wider %>% 
  pivot_longer(names_to = "genus", values_to = "mean_weight", -plot_id)
  
str(surveys_longer)
```

**Challenge 4 (20 min)**

>
1. Spread the surveys data frame with year as columns, plot_id as rows, and the number of genera per plot as the values. You will need to summarize before reshaping, and use the function n_distinct() to get the number of unique genera within a particular chunk of data. It’s a powerful function! See ?n_distinct for more.
2. Now take that data frame and gather() it again, so each row is a unique plot_id by year combination.
3. The surveys data set has two measurement columns: hindfoot_length and weight. This makes it difficult to do things like look at the relationship between mean values of each measurement per year in different plot types. Let’s walk through a common solution for this type of problem. First, use gather() to create a dataset where we have a key column called measurement and a value column that takes on the value of either hindfoot_length or weight. Hint: You’ll need to specify which columns are being gathered.
4. With this new data set, calculate the average of each measurement in each year for each different plot_type. Then spread() them into a data set with a column for hindfoot_length and weight. Hint: You only need to specify the key and value columns for spread().

You can see another example of tidying up the data in example of [pivot_longer](https://tidyr.tidyverse.org/reference/pivot_longer.html), [pivot_wider](https://tidyr.tidyverse.org/reference/pivot_wider.html), or complex example  [here](https://www.brodrigues.co/blog/2019-03-20-pivot/).

#### Exporting data

Writing up the data into new file after spreading or gathering may be needed at some point. We can use the `write_csv()` funtion to generate the CSV files from the dataframe. if you want to write in the other delimiter, you can use `write_delim()` instead. Read the options in `?write_csv` or `?write_delim` for better understanding.


## More excercise

See notebooks here:
<https://www.kaggle.com/rtatman/getting-started-in-r-load-data-into-r>.

## More datasets, more challenge

- Pokemon Dataset : <https://www.kaggle.com/rounakbanik/pokemon/discussion>
- House Price: <https://www.kaggle.com/c/house-prices-advanced-regression-techniques>

## Further Reading

* The core of R (deep understanding how R work): <https://adv-r.hadley.nz/>
* R for Data Science: <https://r4ds.had.co.nz/>
* Another source for R for Data Science: <https://livebook.datascienceheroes.com/>
* Rmarkdown for reproducible documentation: <https://bookdown.org/yihui/rmarkdown/>
* R graph cookbook: <https://r-graphics.org/>
* Feeling gig for Regex (regular expression): <http://www.grymoire.com/Unix/Regular.html>
